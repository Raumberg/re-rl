{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d642195d-2ea2-4c8c-beab-7905175739fe",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—É—Ç–±—É–∫–∞ –æ—Ç unsloth\n",
    "\n",
    "https://unsloth.ai/blog/r1-reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1aac2",
   "metadata": {},
   "source": [
    "–°–¥–µ–ª–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—É—Ç–±—É–∫–∞ –æ—Ç unsloth\n",
    "\n",
    "https://unsloth.ai/blog/r1-reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc6a55-5079-44e6-9502-5949c490eb87",
   "metadata": {},
   "source": [
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏  \n",
    "unsloth - –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏  \n",
    "vllm - –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–¥–µ–ª–∏  \n",
    "tensorboard - –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏  \n",
    "trl - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c006f2-01a1-4aaf-a0ad-f2636a6ef8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unsloth vllm tensorboard\n",
    "# !pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73916e03-c264-4c6c-b222-592484219108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-25 21:32:12 [__init__.py:256] Automatically detected platform cuda.\n",
      "WARNING 03-25 21:32:12 [cuda.py:394] Detected different devices in the system: NVIDIA GeForce RTX 3090 Ti, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á:\n",
    "from re_rl.tasks.generators import (\n",
    "    generate_random_linear_task,\n",
    "    generate_random_futoshiki_task,\n",
    "    generate_random_knights_knaves_task,\n",
    "    # ... –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ ...\n",
    ")\n",
    "# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é compute_reward_for_task (–∏–ª–∏ –∞–Ω–∞–ª–æ–≥),\n",
    "# –∫–æ—Ç–æ—Ä–∞—è —É–º–µ–µ—Ç –ø—Ä–æ–≤–µ—Ä—è—Ç—å –æ—Ç–≤–µ—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ task_type:\n",
    "from re_rl.rewards import compute_reward_for_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afab35",
   "metadata": {},
   "source": [
    "–ú—ã —Ö–æ—Ç–∏–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å GRPO –¥–æ–º–∞ –Ω–∞ 3090/4090 —Å 24–ì–ë –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏.\n",
    "–ë—É–¥–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –≤—Å—é –º–æ–¥–µ–ª—å, –∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä. –í —Ç–∞–∫–æ–º —Ä–µ–∂–∏–º–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç—Å—è, –∞ —Ç—Ä–µ–Ω–∏—Ä—É—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Ü–µ–ª–µ–≤—ã–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–° –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏ unsloth –¥–ª—è GRPO —Å—Ç–∞–ª–æ –≤–æ–∑–º–æ–∂–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø—Ä—è–º–æ —Å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –í 3090 –≤–ª–µ–∑–∞–ª–∞ 3B –º–æ–¥–µ–ª—å —Å 15000 –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n",
    "\n",
    "–í–∞—Ä—å–∏—Ä—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã max_seq_len, gpu_memory_utilization –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é –≤ –ø–∞–º—è—Ç—å –Ω–µ –≤–ª–µ–∑–∞—é—Ç.\n",
    "–ê –≤–æ–æ–±—â–µ - –≤–∞—Ä—å–∏—Ä—É–π—Ç–µ –≤—Å–µ –∏ —Ä–µ—Å–µ—á—å—Ç–µ)\n",
    "\n",
    "1.5B –º–æ–¥–µ–ª—å–∫–∞ —Å –æ–±—â–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º 456 –±—É–¥–µ—Ç —Ç—Ä–µ–Ω–∏—Ç—å—Å—è –Ω–∞ –≤—Å–µ–º —Å–µ—Ç–µ GSM8K-ru –ø—Ä–∏–º–µ—Ä–Ω–æ ~ —á–∞—Å–æ–≤.\n",
    "+ –µ—Å–ª–∏ –≤–∫–ª—é—á–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –µ–≤–∞–ª –Ω–∞ —Ç–µ—Å—Ç—Å–µ—Ç–µ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω –∑–∞–Ω–∏–º–∞–µ—Ç –º–∏–Ω—É—Ç 40-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d6960d2-bfb6-4ab0-85cd-8d0ca81b3cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 2. Max memory: 23.677 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading /media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/ with actual GPU utilization = 43.24%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.68 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2000. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.22 GB. Also swap space = 5 GB.\n",
      "INFO 03-25 21:32:20 [config.py:583] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 03-25 21:32:20 [arg_utils.py:1765] --quantization bitsandbytes is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-25 21:32:20 [llm_engine.py:241] Initializing a V0 LLM engine (v0.8.1) with config: model='/media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/', speculative_config=None, tokenizer='/media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 03-25 21:32:20 [cuda.py:285] Using Flash Attention backend.\n",
      "INFO 03-25 21:32:21 [parallel_state.py:967] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 03-25 21:32:21 [model_runner.py:1110] Starting to load model /media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/...\n",
      "INFO 03-25 21:32:21 [loader.py:1137] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1ca4b8c25049968a68d0b686546402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 21:32:21 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-25 21:32:21 [model_runner.py:1146] Model loading took 1.2130 GB and 0.448007 seconds\n",
      "INFO 03-25 21:32:22 [worker.py:267] Memory profiling takes 0.98 seconds\n",
      "INFO 03-25 21:32:22 [worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.43) = 10.24GiB\n",
      "INFO 03-25 21:32:22 [worker.py:267] model weights take 1.21GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 7.93GiB.\n",
      "INFO 03-25 21:32:23 [executor_base.py:111] # cuda blocks: 18554, # CPU blocks: 11702\n",
      "INFO 03-25 21:32:23 [executor_base.py:116] Maximum concurrency for 2000 tokens per request: 148.43x\n",
      "INFO 03-25 21:32:24 [model_runner.py:1442] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:14<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-25 21:32:38 [model_runner.py:1570] Graph capturing finished in 14 secs, took 2.83 GiB\n",
      "INFO 03-25 21:32:38 [llm_engine.py:447] init engine (profile, create kv cache, warmup model) took 17.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2000 # –ø–∞—Ä–∞–º–µ—Ç—Ä –∑–∞–¥–∞–µ—Ç –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏. –ß–µ–º –±–æ–ª—å—à–µ —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏ –±—É–¥–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –∏ –º–µ–¥–ª–µ–Ω–Ω–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è\n",
    "lora_rank = 64 # LoRA —Ä–∞–Ω–≥ 64 - –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à–æ–π, —É –Ω–∞—Å –ø–æ–ª—É—á–∏—Ç—Å—è ~120 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Ç—Ä–µ–Ω–∏—Ä—É–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\" # –ë–æ–ª—å—à–∞—è 7B –º–æ–¥–µ–ª—å\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct\" # 3B –º–æ–¥–µ–ª—å\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 1.5B –º–æ–¥–µ–ª—å\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" # 0.5B –º–æ–¥–µ–ª—å, —Å–∞–º–∞—è —Å–ª–∞–±–∞—è, –Ω–æ –±—ã—Å—Ç—Ä–µ–µ –≤—Å–µ–≥–æ —É—á–∏—Ç—Å—è\n",
    "\n",
    "model_name = \"/media/user/My Passport2/hfmodels/Qwen2.5-1.5B-Instruct/\"\n",
    "# –æ–¥–∏–Ω –∏–∑ –≤–∞–∂–Ω–µ–π—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞–ª–µ–µ - gpu_memory_utilization.\n",
    "# —Ä–∞—Å—á–µ—Ç—ã –∏–∑ —Ç–æ–≥–æ —á—Ç–æ —É –Ω–∞—Å –¥–æ—Å—Ç—É–ø–Ω–æ 24–ì–ë –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏. –ï—Å–ª–∏ –º–µ–Ω—å—à–µ –∏–ª–∏ –±–æ–ª—å—à–µ - –≤–∞—Ä—å–∏—Ä—É–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ 4-–±–∏—Ç —Ä–µ–∂–∏–º–µ\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # —Å–∫–æ–ª—å–∫–æ –ø–∞–º—è—Ç–∏ –±—É–¥–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–µ, –º–æ–∂–Ω–æ –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # —Å–ø–∏—Å–æ–∫ –º–æ–¥—É–ª–µ–π –∫ –∫–æ—Ç–æ—Ä—ã–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è LoRA\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53cc135-4bda-42b3-bb37-0b87b72910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# –®–∞–≥ 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á\n",
    "###################################################\n",
    "\n",
    "# –î–æ–ø—É—Å—Ç–∏–º, –º—ã —Ö–æ—Ç–∏–º –ø–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –õ–ò–ù–ï–ô–ù–´–• —É—Ä–∞–≤–Ω–µ–Ω–∏—è—Ö (–∫–∞–∫ –ø—Ä–∏–º–µ—Ä).\n",
    "# –ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Å–¥–µ–ª–∞–µ—Ç N —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –∏ M –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö:\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from re_rl.rewards import (\n",
    "    reward_format_check,\n",
    "    reward_cot_quality,\n",
    "    reward_correctness,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def make_linear_dataset(num_train=200, num_eval=50, language=\"ru\", detail_level=3):\n",
    "    train_items = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_linear_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è\n",
    "        train_items.append({\n",
    "            \"task_type\": t.get_task_type(),       # \"linear\"\n",
    "            \"problem\": r[\"problem\"],             # –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏\n",
    "            \"prompt\": r[\"prompt\"],               # \"–ó–∞–¥–∞—á–∞: ...\"\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    eval_items = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_linear_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        eval_items.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return train_items, eval_items\n",
    "\n",
    "# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å make_futoshiki_dataset, make_knights_knaves_dataset, ...\n",
    "# –ò–ª–∏ –∂–µ –≤—Å–µ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É —Ñ—É–Ω–∫—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º.\n",
    "\n",
    "\n",
    "###################################################\n",
    "# –®–∞–≥ 4: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç unsloth\n",
    "###################################################\n",
    "SYSTEM_PROMPT = \"\"\"–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n",
    "<reasoning>\n",
    "(–®–∞–≥–∏ —Ä–µ—à–µ–Ω–∏—è)\n",
    "</reasoning>\n",
    "<answer>\n",
    "(–ö–æ—Ä–æ—Ç–∫–∏–π –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç–≤–µ—Ç)\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_unsloth_format(dataset_list: List[Dict]):\n",
    "    \"\"\"\n",
    "    –ù–∞ –≤—Ö–æ–¥: list, –≥–¥–µ –∫–∞–∂–¥—ã–π dict –∏–º–µ–µ—Ç \n",
    "      {\n",
    "       \"task_type\": str, \n",
    "       \"problem\": str,\n",
    "       \"prompt\": str,\n",
    "       \"final_answer\": str,\n",
    "       ...\n",
    "      }\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫, –ø—Ä–∏–≥–æ–¥–Ω—ã–π –¥–ª—è GRPOTrainer: \n",
    "      [\n",
    "        {\n",
    "          \"prompt\": [\n",
    "             {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "             {\"role\": \"user\",   \"content\": item[\"prompt\"],\n",
    "               \"metadata\": {\n",
    "                  \"task_type\": item[\"task_type\"],\n",
    "                  \"problem\": item[\"problem\"],\n",
    "                  \"ref_final_answer\": item[\"final_answer\"]\n",
    "                }\n",
    "             }\n",
    "          ],\n",
    "          \"answer\": item[\"final_answer\"]\n",
    "        },\n",
    "        ...\n",
    "      ]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for d in dataset_list:\n",
    "        user_msg = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": d[\"prompt\"],\n",
    "            \"metadata\": {\n",
    "                \"task_type\": d[\"task_type\"],\n",
    "                \"problem\": d[\"problem\"],\n",
    "                \"ref_final_answer\": d[\"final_answer\"]\n",
    "            }\n",
    "        }\n",
    "        system_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT\n",
    "        }\n",
    "        ex = {\n",
    "            \"prompt\": [system_msg, user_msg],\n",
    "            \"answer\": d[\"final_answer\"]  # \"gold\"\n",
    "        }\n",
    "        out.append(ex)\n",
    "    return out\n",
    "\n",
    "###################################################\n",
    "# –®–∞–≥ 5: —Å–ø–∏—Å–æ–∫ reward-—Ñ—É–Ω–∫—Ü–∏–π, –≤—ã–∑—ã–≤–∞—é—â–∞—è\n",
    "###################################################\n",
    "reward_funcs_list = [\n",
    "    reward_format_check,   # –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –µ—Å—Ç—å <reasoning>...</reasoning> –∏ <answer>...</answer>\n",
    "    reward_cot_quality,    # –Ω–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π reasoning\n",
    "    reward_correctness,    # –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ\n",
    "]\n",
    "\n",
    "\n",
    "###################################################\n",
    "# –®–∞–≥ 6: –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –æ–±—É—á–µ–Ω–∏–µ GRPO –Ω–∞ –ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö\n",
    "###################################################\n",
    "# 1) –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "train_lin, eval_lin = make_linear_dataset(num_train=1000, num_eval=50, language=\"ru\", detail_level=3)\n",
    "# 2) –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º\n",
    "train_lin_unsloth = convert_to_unsloth_format(train_lin)\n",
    "eval_lin_unsloth  = convert_to_unsloth_format(eval_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e56109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lin_unsloth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe3b30b-1f0c-4792-8c67-41465327da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 125\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 4 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 73,859,072/5,000,000,000 (1.48% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 41:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completion_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / reward_format_check</th>\n",
       "      <th>rewards / reward_cot_quality</th>\n",
       "      <th>rewards / reward_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795781</td>\n",
       "      <td>0.468305</td>\n",
       "      <td>162.153125</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.169063</td>\n",
       "      <td>0.085469</td>\n",
       "      <td>0.541250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.341033</td>\n",
       "      <td>154.942187</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.198750</td>\n",
       "      <td>0.099688</td>\n",
       "      <td>0.639062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.095625</td>\n",
       "      <td>0.429912</td>\n",
       "      <td>147.239062</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.198125</td>\n",
       "      <td>0.099688</td>\n",
       "      <td>0.797812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.253750</td>\n",
       "      <td>0.432612</td>\n",
       "      <td>140.735938</td>\n",
       "      <td>0.023145</td>\n",
       "      <td>0.196563</td>\n",
       "      <td>0.099063</td>\n",
       "      <td>0.958125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.375313</td>\n",
       "      <td>0.326483</td>\n",
       "      <td>132.918750</td>\n",
       "      <td>0.042795</td>\n",
       "      <td>0.197813</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.470469</td>\n",
       "      <td>0.265438</td>\n",
       "      <td>125.031250</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>0.199063</td>\n",
       "      <td>0.099844</td>\n",
       "      <td>1.171562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.478438</td>\n",
       "      <td>0.249083</td>\n",
       "      <td>138.023438</td>\n",
       "      <td>0.044940</td>\n",
       "      <td>0.199375</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.179062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.483750</td>\n",
       "      <td>0.242816</td>\n",
       "      <td>140.032813</td>\n",
       "      <td>0.041594</td>\n",
       "      <td>0.199688</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.184062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.530625</td>\n",
       "      <td>0.157024</td>\n",
       "      <td>134.068750</td>\n",
       "      <td>0.039088</td>\n",
       "      <td>0.199688</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.230937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.511563</td>\n",
       "      <td>0.184734</td>\n",
       "      <td>136.468750</td>\n",
       "      <td>0.043080</td>\n",
       "      <td>0.199688</td>\n",
       "      <td>0.099844</td>\n",
       "      <td>1.212031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>1.517188</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>132.153125</td>\n",
       "      <td>0.036694</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.217187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.520938</td>\n",
       "      <td>0.178152</td>\n",
       "      <td>133.279687</td>\n",
       "      <td>0.043859</td>\n",
       "      <td>0.199688</td>\n",
       "      <td>0.099844</td>\n",
       "      <td>1.221406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.001271246774122119, metrics={'train_runtime': 2543.5643, 'train_samples_per_second': 0.393, 'train_steps_per_second': 0.049, 'total_flos': 0.0, 'train_loss': 0.001271246774122119})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "# 3) Config\n",
    "train_args_lin = GRPOConfig(\n",
    "    use_vllm = True,\n",
    "    vllm_gpu_memory_utilization = 0.3,\n",
    "    learning_rate = 2e-5,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 10,\n",
    "    save_steps = 50,\n",
    "    bf16 = True,    # –µ—Å–ª–∏ GPU –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç bf16\n",
    "    fp16 = False,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_generations = 8,\n",
    "    max_prompt_length = 1000,\n",
    "    max_completion_length = 1000,\n",
    "    output_dir = \"outputs_linear_only\",\n",
    ")\n",
    "\n",
    "trainer_lin = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = reward_funcs_list,  # —Å–ø–∏—Å–æ–∫ –∏–∑ –æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "    args = train_args_lin,\n",
    "    train_dataset = train_lin_unsloth,\n",
    "    eval_dataset  = eval_lin_unsloth,\n",
    ")\n",
    "trainer_lin.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee36d2-e3da-4e84-8d38-559fc68f7d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.50it/s, est. speed input: 349.83 toks/s, output: 153.57 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'–í —Å–ª–æ–≤–µ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\" —Ä –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 2 —Ä–∞–∑–∞.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ä –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–µ —Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a5a5e5d-68fe-4091-95f2-5c1df87be580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cffb93d-c3fa-4b0b-ba0d-b1f99fd3609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 68.68 toks/s, output: 110.48 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<reasoning>\\n1. –ù–∞—á–Ω–µ–º —Å –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–≤–∞ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\".\\n2. –í —Å–ª–æ–≤–µ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\" –º—ã –º–æ–∂–µ–º —É–≤–∏–¥–µ—Ç—å –±—É–∫–≤—É \"—Ä\" –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –º–µ—Å—Ç–∞—Ö:\\n   - –í –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\"\\n   - –í –∫–æ–Ω—Ü–µ —Å–ª–æ–≤–∞ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\"\\n3. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –±—É–∫–≤–∞ \"—Ä\" –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–µ \"—Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏\" 2 —Ä–∞–∑–∞.\\n</reasoning>\\n<answer>\\n2\\n</answer>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"–°–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ä –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Å–ª–æ–≤–µ —Å—Ç—Ä–∞–≤–±–µ—Ä—Ä–∏?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbb3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
