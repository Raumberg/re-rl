{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d642195d-2ea2-4c8c-beab-7905175739fe",
   "metadata": {},
   "source": [
    "Сделано на основе ноутбука от unsloth\n",
    "\n",
    "https://unsloth.ai/blog/r1-reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1aac2",
   "metadata": {},
   "source": [
    "Сделано на основе ноутбука от unsloth\n",
    "\n",
    "https://unsloth.ai/blog/r1-reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acc6a55-5079-44e6-9502-5949c490eb87",
   "metadata": {},
   "source": [
    "Установим все необходимые библиотеки  \n",
    "unsloth - Для оптимизации тренировки  \n",
    "vllm - для инференса модели  \n",
    "tensorboard - для логирования и визуализации  \n",
    "trl - библиотека для тренировки LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c006f2-01a1-4aaf-a0ad-f2636a6ef8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth vllm tensorboard\n",
    "!pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73916e03-c264-4c6c-b222-592484219108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, PatchFastRL\n",
    "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
    "# Импортируем наши генераторы конкретных типов задач:\n",
    "from re_rl.tasks.generators import (\n",
    "    generate_random_linear_task,\n",
    "    generate_random_quadratic_task,\n",
    "    generate_random_futoshiki_task,\n",
    "    generate_random_knights_knaves_task,\n",
    "    generate_random_contradiction_task,\n",
    "    generate_random_urn_probability_task,\n",
    "    # ... при желании остальные ...\n",
    ")\n",
    "# Импортируем глобальную функцию compute_reward_for_task (или аналог),\n",
    "# которая умеет проверять ответ для каждого task_type:\n",
    "from re_rl.rewards import (\n",
    "    reward_format_check,\n",
    "    reward_cot_quality,\n",
    "    reward_correctness,\n",
    ")\n",
    "# Импортируем prompts, чтобы подтянуть инструкции\n",
    "from re_rl.tasks.prompts import PROMPT_TEMPLATES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40afab35",
   "metadata": {},
   "source": [
    "Мы хотим попробовать потренировать GRPO дома на 3090/4090 с 24ГБ видеопамяти.\n",
    "Будем тренировать не всю модель, а LoRA адаптер. В таком режиме веса модели замораживаются, а тренируются дополнительные матрицы, которые затем будут добавлены в целевые веса модели.\n",
    "\n",
    "С последними обновлениями unsloth для GRPO стало возможным использовать модели прямо с очень большим контекстом. В 3090 влезала 3B модель с 15000 контекстом.\n",
    "\n",
    "Варьируйте параметры max_seq_len, gpu_memory_utilization если параметры установленные по-умолчанию в память не влезают.\n",
    "А вообще - варьируйте все и ресечьте)\n",
    "\n",
    "1.5B моделька с общим контекстом 456 будет трениться на всем сете GSM8K-ru примерно ~ часов.\n",
    "+ если включать промежуточный евал на тестсете один прогон занимает минут 40-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6960d2-bfb6-4ab0-85cd-8d0ca81b3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2000 # параметр задает длину контекста модели. Чем больше тем больше памяти будет требоваться и медленне тренироваться\n",
    "lora_rank = 64 # LoRA ранг 64 - довольно большой, у нас получится ~120 миллионов тренируемых параметров.\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\" # Большая 7B модель\n",
    "# model_name = \"Qwen/Qwen2.5-3B-Instruct\" # 3B модель\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 1.5B модель\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" # 0.5B модель, самая слабая, но быстрее всего учится\n",
    "# один из важнейших параметров далее - gpu_memory_utilization.\n",
    "# расчеты из того что у нас доступно 24ГБ видеопамяти. Если меньше или больше - варьируйте значение.\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # Загружаем модель в 4-бит режиме\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.5, # сколько памяти будет занимать модель на видеокарте, можно варьировать\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # список модулей к которым применяется LoRA\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53cc135-4bda-42b3-bb37-0b87b72910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# Шаг 3: Генерация задач\n",
    "###################################################\n",
    "\n",
    "# Допустим, мы хотим потренироваться на ЛИНЕЙНЫХ уравнениях (как пример).\n",
    "# Напишем функцию, которая сделает N тренировочных и M валидационных:\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "# 1) Универсальная функция: build_prompt_for_task\n",
    "def build_prompt_for_task(task_info: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Принимает словарь task_info, в котором есть, как минимум:\n",
    "      {\n",
    "        \"task_type\": str,\n",
    "        \"language\": str,   # 'ru' или 'en'\n",
    "        \"problem\": str,\n",
    "        \"final_answer\": str,\n",
    "        ... (остальные поля при желании)\n",
    "      }\n",
    "    1) Из prompts берем инструкции: PROMPT_TEMPLATES[task_type][\"instructions\"][language]\n",
    "    2) Формируем system-msg = инструкции\n",
    "    3) Формируем user-msg = problem (текст задачи) + metadata\n",
    "    Возвращаем список сообщений [system_msg, user_msg].\n",
    "    \"\"\"\n",
    "    # Шаг 1: Берём task_type, language\n",
    "    task_type = task_info[\"task_type\"]\n",
    "    language = task_info[\"language\"]\n",
    "\n",
    "    # Шаг 2: Достаем instructions\n",
    "    instructions = \"\"\n",
    "    if task_type in PROMPT_TEMPLATES:\n",
    "        instructions_dict = PROMPT_TEMPLATES[task_type].get(\"instructions\", {})\n",
    "        instructions = instructions_dict.get(language, \"\")\n",
    "\n",
    "    # Вы можете дополнительно объединить base_system_text + instructions.\n",
    "    # Пока вставляем только instructions.\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instructions.strip()\n",
    "    }\n",
    "\n",
    "    # Шаг 3: user msg (условие задачи)\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": task_info[\"problem\"],\n",
    "        \"metadata\": {\n",
    "            \"task_type\": task_type,\n",
    "            \"problem\": task_info[\"problem\"],\n",
    "            \"ref_final_answer\": task_info[\"final_answer\"]\n",
    "        }\n",
    "    }\n",
    "    return [system_msg, user_msg]\n",
    "\n",
    "\n",
    "# Аналогично можете сделать make_futoshiki_dataset, make_knights_knaves_dataset, ...\n",
    "# Или же все объединить в одну функцию с параметром.\n",
    "\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Отвечай строго в формате:\n",
    "<reasoning>\n",
    "(Шаги решения)\n",
    "</reasoning>\n",
    "<answer>\n",
    "(Короткий итоговый ответ)\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "# Шаг 4: Преобразование в нужный формат\n",
    "###################################################\n",
    "\n",
    "def convert_to_trl_format(dataset_list: List[Dict]):\n",
    "    \"\"\"\n",
    "    Пробегаемся по всем элементам dataset_list,\n",
    "    вызываем build_prompt_for_task(d), \n",
    "    и формируем структуру для GRPOTrainer:\n",
    "      {\n",
    "        \"prompt\": [...],\n",
    "        \"answer\": d[\"final_answer\"]\n",
    "      }\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for d in dataset_list:\n",
    "        # формируем prompt\n",
    "        prompt_msgs = build_prompt_for_task(d)\n",
    "        ex = {\n",
    "            \"prompt\": prompt_msgs,\n",
    "            \"answer\": d[\"final_answer\"]\n",
    "        }\n",
    "        out.append(ex)\n",
    "    return out\n",
    "\n",
    "###################################################\n",
    "# список reward-функций, вызывающая\n",
    "###################################################\n",
    "reward_funcs_list = [\n",
    "    reward_format_check,   # проверка, что есть <reasoning>...</reasoning> и <answer>...</answer>\n",
    "    reward_cot_quality,    # небольшой бонус за содержательный reasoning\n",
    "    reward_correctness,    # основная награда за правильное решение\n",
    "]\n",
    "\n",
    "#############################\n",
    "# линейные уравнения\n",
    "#############################\n",
    "def make_linear_dataset(num_train=200, num_eval=50, language=\"ru\", detail_level=3):\n",
    "    train_items = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_linear_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        # Сохраняем нужные поля\n",
    "        train_items.append({\n",
    "            \"task_type\": t.get_task_type(),       # \"linear\"\n",
    "            \"problem\": r[\"problem\"],             # описание задачи\n",
    "            \"prompt\": r[\"prompt\"],               # \"Задача: ...\"\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    eval_items = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_linear_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        eval_items.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return train_items, eval_items\n",
    "\n",
    "#############################\n",
    "# квадратичные уравнения\n",
    "#############################\n",
    "def make_quadratic_dataset(num_train=200, num_eval=50, language=\"ru\", detail_level=3):\n",
    "    tr = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_quadratic_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        tr.append({\n",
    "            \"task_type\": t.get_task_type(),  # \"quadratic\"\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    ev = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_quadratic_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        ev.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return tr, ev\n",
    "\n",
    "#############################\n",
    "# FutoshikiTask\n",
    "#############################\n",
    "\n",
    "def make_futoshiki_dataset(num_train=100, num_eval=20, language=\"ru\", detail_level=5):\n",
    "    train_list = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_futoshiki_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        train_list.append({\n",
    "            \"task_type\": t.get_task_type(),  # \"futoshiki\"\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    eval_list = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_futoshiki_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        eval_list.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return train_list, eval_list\n",
    "\n",
    "#############################\n",
    "# KnightsKnavesTask\n",
    "#############################\n",
    "def make_knights_knaves_dataset(num_train=100, num_eval=20, language=\"en\", detail_level=4):\n",
    "    tr = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_knights_knaves_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        tr.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"language\": language,   # <--- добавили\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    ev = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_knights_knaves_task(language=language, detail_level=detail_level)\n",
    "        r = t.get_result()\n",
    "        ev.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"language\": language,   # <--- добавили\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return tr, ev\n",
    "\n",
    "#############################\n",
    "# ContradictionTask (пример)\n",
    "#############################\n",
    "def make_contradiction_dataset(num_train=100, num_eval=20, language=\"ru\"):\n",
    "    tr = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_contradiction_task(language=language, num_statements=10)\n",
    "        r = t.get_result()\n",
    "        tr.append({\n",
    "            \"task_type\": t.get_task_type(),  # \"contradiction\"\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    ev = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_contradiction_task(language=language, num_statements=10)\n",
    "        r = t.get_result()\n",
    "        ev.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return tr, ev\n",
    "#############################\n",
    "# Шаг 8: UrnProbabilityTask (пример)\n",
    "#############################\n",
    "def make_urnprob_dataset(num_train=80, num_eval=20, language=\"ru\"):\n",
    "    tr = []\n",
    "    for _ in range(num_train):\n",
    "        t = generate_random_urn_probability_task(language=language)\n",
    "        r = t.get_result()\n",
    "        tr.append({\n",
    "            \"task_type\": t.get_task_type(),  # \"urn_probability\"\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    ev = []\n",
    "    for _ in range(num_eval):\n",
    "        t = generate_random_urn_probability_task(language=language)\n",
    "        r = t.get_result()\n",
    "        ev.append({\n",
    "            \"task_type\": t.get_task_type(),\n",
    "            \"problem\": r[\"problem\"],\n",
    "            \"prompt\": r[\"prompt\"],\n",
    "            \"solution_steps\": r[\"solution_steps\"],\n",
    "            \"final_answer\": r[\"final_answer\"],\n",
    "        })\n",
    "    return tr, ev\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Шаг 6: Собственно обучение GRPO на выбранных\n",
    "###################################################\n",
    "# 1) генерируем датасет\n",
    "# train_set, eval_set = make_linear_dataset(num_train=1000, num_eval=50, language=\"ru\", detail_level=3)\n",
    "\n",
    "# train_set, eval_set = make_quadratic_dataset(num_train=1000, num_eval=50)\n",
    "\n",
    "# train_set, eval_set = make_futoshiki_dataset(num_train=500, num_eval=20)\n",
    "\n",
    "train_set, eval_set = make_knights_knaves_dataset(num_train=500, num_eval=20, language=\"ru\", detail_level=4)\n",
    "\n",
    "# train_set, eval_set = make_contradiction_dataset(num_train=80, num_eval=20, language=\"ru\")\n",
    "\n",
    "# train_set, eval_set = make_urnprob_dataset(num_train=80, num_eval=20, language=\"ru\")\n",
    "\n",
    "# 2) конвертируем\n",
    "train_converted_set = convert_to_trl_format(train_set)\n",
    "eval_converted_set  = convert_to_trl_format(eval_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_converted_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe3b30b-1f0c-4792-8c67-41465327da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, LogCompletionsCallback\n",
    "# 3) Config\n",
    "train_args_lin = GRPOConfig(\n",
    "    use_vllm = True,\n",
    "    vllm_gpu_memory_utilization = 0.3,\n",
    "    learning_rate = 2e-5,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    save_steps = 50,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_generations = 8,\n",
    "    max_prompt_length = 1000,\n",
    "    max_completion_length = 1000,\n",
    "    report_to = \"tensorboard\", # пишем логи в тензорборд\n",
    "    output_dir = \"outputs_artificial_task\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = reward_funcs_list,  # список из одной функции\n",
    "    args = train_args_lin,\n",
    "    train_dataset = train_converted_set,\n",
    "    eval_dataset  = eval_converted_set,\n",
    ")\n",
    "\n",
    "completions_callback = LogCompletionsCallback(trainer=trainer)\n",
    "trainer.add_callback(completions_callback)\n",
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdee36d2-e3da-4e84-8d38-559fc68f7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"Сколько раз р встречается в слове стравберри?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a5e5d-68fe-4091-95f2-5c1df87be580",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffb93d-c3fa-4b0b-ba0d-b1f99fd3609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n",
    "    {\"role\" : \"user\", \"content\" : \"Сколько раз р встречается в слове стравберри?\"},\n",
    "], tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbbb3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "re_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
